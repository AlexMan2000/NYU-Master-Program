{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T02:05:32.664122Z",
     "start_time": "2019-09-17T02:05:32.626709Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def data_loader():\n",
    "    # load data\n",
    "    with open('train','r') as file:\n",
    "        train_data = file.read().split('\\n')[:-1]\n",
    "    with open('test','r') as file:\n",
    "        test_data = file.read().split('\\n')[:-1]\n",
    "    return train_data, test_data\n",
    "\n",
    "def parser(datum):\n",
    "    # extract labels and words\n",
    "    email_addr, label, words = datum.split(' ',2)\n",
    "    words = words.split()\n",
    "    # transform words into dictionary\n",
    "    word_dict = dict(zip([words[i] for i in range(0, len(words), 2)], [int(words[i+1]) for i in range(0, len(words), 2)]))\n",
    "    # transform label into 0, 1\n",
    "    if label == 'ham':\n",
    "        label = 0\n",
    "    elif label == 'spam':\n",
    "        label = 1\n",
    "    else: \n",
    "        raise ValueError\n",
    "    return label, word_dict\n",
    "\n",
    "def data_preprocessing(train_data, test_data):\n",
    "    y_train = np.zeros(len(train_data))\n",
    "    y_test = np.zeros(len(test_data))\n",
    "    x_train = []\n",
    "    x_test = []\n",
    "    for i, datum in enumerate(train_data):\n",
    "        label, word_dict = parser(datum)\n",
    "        y_train[i] = label\n",
    "        x_train.append(word_dict)\n",
    "    for i, datum in enumerate(test_data):\n",
    "        label, word_dict = parser(datum)\n",
    "        y_test[i] = label\n",
    "        x_test.append(word_dict)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "def compute_empirical_pmf_y(y_train):\n",
    "    # compte distribution P(y=1), P(y=0)\n",
    "    # TODO\n",
    "    return np.sum(y_train == 1) / len(y_train),  np.sum(y_train == 0) / len(y_train)\n",
    "\n",
    "def m_estimation_conditional_probability(x_train_frt, y_train, num_vocab, m):\n",
    "    # compute P(x_j|y=1) and P(x_j|y=0) for j = 1, ..., d\n",
    "    # TODO\n",
    "    p_on_spam = np.zeros((2, x_train_frt.shape[1]))\n",
    "    p_on_ham = np.zeros((2, x_train_frt.shape[1]))\n",
    "    \n",
    "    temp_df = pd.DataFrame(np.hstack([x_train_frt,y_train.reshape(-1,1)]))\n",
    "    temp_df.columns = temp_df.columns = [*list(temp_df.columns)[:-1],\"label\"]\n",
    "    \n",
    "    for j in range(num_vocab):\n",
    "        column_data_0 = (temp_df[temp_df[\"label\"]==0])[j]\n",
    "        column_data_1 = (temp_df[temp_df[\"label\"]==1])[j]\n",
    "        \n",
    "        # P(x_j = 1|y=0) by definition\n",
    "        p_on_ham[1, j] = (sum(column_data_0 > 0) + m) / (len(column_data_0) + m * num_vocab)\n",
    "        # P(x_j = 0|y=0) by the fact that conditional probability is a valid pmf\n",
    "        p_on_ham[0, j] = 1 - p_on_ham[1, j]\n",
    "                                                          \n",
    "        \n",
    "\n",
    "        # P(x_j = 1|y=1)\n",
    "        p_on_spam[1, j] = (sum(column_data_1 > 0) + m) / (len(column_data_1) + m * num_vocab)\n",
    "        # P(x_j = 0|y=1)\n",
    "        p_on_spam[0, j] = 1 - p_on_spam[1, j]\n",
    "                                                          \n",
    "    return p_on_spam, p_on_ham\n",
    "                                                          \n",
    "\n",
    "def log_estimated_probability(p_spam, p_ham, p_on_spam_m, p_on_ham_m, x_frts):\n",
    "    # compute log(P(y=1, x_1, x_2,..., x_n)) and log(P(y=0, x_1, x_2,..., x_n))\n",
    "    # hint: log(P(y, x_1, x_2,..., x_n)) = log(P(y=1)) + log(P(x_1 | y)) + ... + log(P(x_d | y))\n",
    "    # TODO\n",
    "    \n",
    "    log_p_spam, log_p_ham = np.zeros((len(x_frts), )), np.zeros((len(x_frts), ))\n",
    "    \n",
    "    for i in range(len(x_frts)):\n",
    "        log_p_spam[i] = np.sum([np.log(p_spam)]+[np.log(p_on_spam_m[1 if x_frts[i, j] > 0 else 0 ,j]) for j in range(x_frts.shape[1])])\n",
    "        log_p_ham[i] = np.sum([np.log(p_ham)]+[np.log(p_on_ham_m[1 if x_frts[i, j] > 0 else 0 ,j]) for j in range(x_frts.shape[1])])\n",
    "    \n",
    "    return log_p_spam,log_p_ham\n",
    "\n",
    "def accuarcy(y_true, y_pred):\n",
    "    # calculate accuracy\n",
    "    # TODO \n",
    "    \n",
    "    return np.sum(y_true == y_pred) / len(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T02:46:56.253606Z",
     "start_time": "2019-09-11T02:46:54.907026Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "## Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T02:05:34.270338Z",
     "start_time": "2019-09-17T02:05:32.712000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 1000) (1000, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "# load data\n",
    "train_data, test_data = data_loader()\n",
    "\n",
    "# extract labels to 0,1 and features to dicticnary\n",
    "x_train, y_train, x_test, y_test = data_preprocessing(train_data, test_data)\n",
    "\n",
    "# transform word dicts to feature vectors\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "x_train_frt = vectorizer.fit_transform(x_train)\n",
    "x_test_frt = vectorizer.transform(x_test)\n",
    "print(x_train_frt.shape, x_test_frt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search for the best $m$. \n",
    "\n",
    "Which $m$ result in the highest accuracy in the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-17T02:05:37.374062Z",
     "start_time": "2019-09-17T02:05:37.364074Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexm\\AppData\\Local\\Temp\\ipykernel_112388\\999649774.py:83: RuntimeWarning: divide by zero encountered in log\n",
      "  log_p_spam[i] = np.sum([np.log(p_spam)]+[np.log(p_on_spam_m[1 if x_frts[i, j] > 0 else 0 ,j]) for j in range(x_frts.shape[1])])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:0.932\n",
      "0.01:0.927\n",
      "0.1:0.924\n",
      "1:0.922\n",
      "10:0.905\n",
      "100:0.88\n",
      "1000:0.845\n"
     ]
    }
   ],
   "source": [
    "def pipeline(x_train_frt, y_train, x_test_frt, y_test, m):\n",
    "    p_spam, p_ham = compute_empirical_pmf_y(y_train)\n",
    "    p_on_spam_m, p_on_ham_m = m_estimation_conditional_probability(x_train_frt, y_train, x_train_frt.shape[1], m)\n",
    "    log_p_spam, log_p_ham = log_estimated_probability(p_spam, p_ham, p_on_spam_m, p_on_ham_m, x_test_frt)\n",
    "    test_pred = (log_p_spam > log_p_ham)\n",
    "    print(str(m) + \":\" + str(accuarcy(y_test, test_pred)))\n",
    "\n",
    "m_grid = [0, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "for m in m_grid:\n",
    "    pipeline(x_train_frt, y_train, x_test_frt, y_test, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "90c54abfee7b04b58581da627331e79059a60a43b608b7e15d6f76f7651b8a78"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
