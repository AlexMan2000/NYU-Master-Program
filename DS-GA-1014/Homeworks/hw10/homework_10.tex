\documentclass[11pt,nocut]{article}

\usepackage{style/packages}
\usepackage{style/notations}
\usepackage{fancyhdr}

\pagestyle{fancy}
\renewcommand{\headrulewidth}{1pt}
\fancyhead[R]{DS-GA 1014---Fall 2023}
\fancyhead[L]{HW10 --- Linear Regression --- due November 29, 2023 at noon}

\setcounter{section}{10}

\begin{document}

\input{style/preamble_homeworks.tex}


\vspace{5mm}

\begin{problem}[2 points]
	Let $A \in \R^{n \times m}$ and $y \in \R^n$. We consider the least squares problem:
	\begin{equation}\label{eq:LS}
		\min_{x \in \R^m} \|Ax - y\|^2\,.
	\end{equation}
	We know from the lecture that $x^{\rm LS} \defeq A^{\dagger}y$ is a solution of \eqref{eq:LS}. 
	\begin{enumerate}[label=\normalfont(\textbf{\alph*})]
		% \item Prove that if $z \in \Ker(A)$, then $(A^{\dagger})^T z = 0$.
		\item Show that $x^{\rm LS} \perp \Ker(A)$.
		\item Deduce that $x^{\rm LS}$ is the solution of \eqref{eq:LS} that has the smallest Euclidean norm. \\
  \textit{(Hint: use the Pythagorean Theorem from Lecture 4)}
	\end{enumerate}
\end{problem}

\vspace{5mm}

\begin{problem}[3 points]
	Let $A \in \R^{n \times d}$ and $y \in \R^n$.
	For a given penalization parameter $\lambda >0$, the Ridge Regression optimization problem is
	\begin{equation}\label{eq:ridge}
		\min_{x \in \R^d}
		\| Ax - y \|^2 + \lambda \|x\|^2 \,.
	\end{equation}
	\begin{enumerate}[label=\normalfont(\textbf{\alph*})]
		\item Prove that $A^{\sT} A + \lambda \Id_d$ is invertible. 
		\item Prove that the solution of~\eqref{eq:ridge} is given by
		$
		x_{\lambda} = (A^{\sT} A + \lambda \Id_d)^{-1} A^{\sT} y.
		$
            \item Given $\lambda_1\ge \lambda_2>0$, show that $\|x_{\lambda_1}\|^2 \le \|x_{\lambda_2}\|^2$. 
            % This confirms our intuition that ridge regularization decreases the norm of the solution. 
	\end{enumerate}
	
\end{problem}

\vspace{5mm}

\begin{problem}[2 points]
In this question we show that the matrix norms introduced in lecture are submultiplicative. For all $A \in \R^{m \times n}$ and $B\in \R^{n \times k}$, prove that
	\begin{enumerate}[label=\normalfont(\textbf{\alph*})]

		\item $\|AB\|_{\rm Sp} \leq \|A\|_{\rm Sp} \|B\|_{\rm Sp}$. 
        \item $\|AB\|_{F} \leq \|A\|_{F} \|B\|_{F}$. 
	\end{enumerate}
\end{problem}

\vspace{5mm}

\begin{problem}[3 points]
	In this problem, you will gets hands-on coding experience with 1) ridge regularization, 2) a more advanced version of regression that involves polynomial fits rather than linear fits. Instructions and questions are in the Jupyter notebook \texttt{polyreg.ipynb}.
	%
	%
	\\ \\ \underline{Details on submission.} Please code in Python and use the provided Jupyter Notebook. Submit as in previous HW assignments (only submit a PDF version of your notebook by right-clicking $\to$ `print' $\to$ `Save as pdf').
\end{problem}

\vspace{5mm}

\begin{problem}[$\star$]
Is it true that for all $n,m,k \geq 1$, all $A \in \R^{n \times m}$ and $B\in \R^{m \times k}$:
			$$
			\|AB\|_{F} \leq \|A\|_{\rm Sp} \|B\|_{F} \ {\rm ?}
			$$
			Give a proof or a counter-example.
\end{problem}

\end{document}
